---
title: "Neural_Networks_with_R"
author: "Odo Luo"
date: "March 24, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


# Homework 3: Neural Networks with R

# 1 and 2 
1. Use the iris data set (data(iris)) with the four features
  a. Sepal.Length
  b. Sepal.Width
  c. Petal.Length
  d. Petal.Width
and the target Species with the three classes setosa, versicolor, and virginica

2. Create train and test data using a fixed split of 70-30



```{r}
library(knitr)
library(datasets)
library(dplyr)
library(neuralnet)
library(caret)
library(dummies)
```

```{r}
data <- iris %>% 
  mutate(id=row_number())  
train <- data %>%sample_frac(.70) 
test <-  anti_join(data, train,'id')
train <- train %>% select(-id)
test <- test %>% select(-id)
```
# 3
3. Use the training set to find a good MLP architecture with:
  a. 1 to 2 hidden layers and
  b. 2 to 5 neurons in the hidden layers
  
and a logistic activation function. Find the best of all 8 possible architectures (2x4 = 8
combinations; grid search with 10-times stratified1
cross validation or use a validation set â€“
80-10-10) and use it in the following tasks.


Creating Folds for 10-Fold Cross Validation:
```{r}
train$Species<-as.factor(train$Species)
train<-train %>% mutate(id=row_number()) 
folds <- createFolds(train$Species)
#train<-cbind(train,dummy(train$Species,sep = "_"))
#train <- train %>% select(-Species)
```

Getting vector "hidden" attribute in neuralnet:
```{r}
getHiddenLayerVector <- function(hiddenLayerNum,neuronsNum){
  hiddenLayer = c()
  for(i in 1:hiddenLayerNum){
    hiddenLayer[i]= neuronsNum
  }
  return (hiddenLayer)
}
```

Calculating mean(accuracy) using 10fold CV:

```{r}
hiddenLayerNum =  2
hiddenLayerNeurons=5

result <- data.frame(layers=integer(),neurons=integer(),accuracy=double())
#1:2 Layer
for( layerNum in 1:hiddenLayerNum){
  
  #2:5 Neurons per Layer
  for(neuronsNum in 2:hiddenLayerNeurons){
    out<-NULL
    
    #CV Iterration
    for(cv in 1:10){
      test_cv= train[folds[[cv]],]
      train_cv= train[-test_cv$id,]
      nn= neuralnet(Species~Sepal.Length+Sepal.Width+Petal.Length,data=train_cv, 
                     hidden=getHiddenLayerVector(layerNum,neuronsNum),
                     act.fct = "logistic",linear.output = FALSE,rep=2,threshold = 0.5)
      prediction <- neuralnet::compute(nn, (test_cv %>% select(-id,-Species)))
      prediction <- max.col(prediction$net.result) %>%  as.factor()
      prediction <- factor(prediction,levels = c(1,2,3))
      real <- test_cv %>%   mutate(Species= ifelse(Species=="setosa",1,ifelse(Species=="versicolor",2,ifelse(Species=="virginica",3,NA))))
      real<- as.factor(real[,"Species"])
      
      c<-confusionMatrix(real,prediction)
      out[cv]<-c$overall[["Accuracy"]]
    }
    result <- rbind(result,data.frame(layerNum,neuronsNum,mean(out)))
  }
}

result
```

# 4 Visualize the best model using the plot function of neuralnet.

Both, one hidden layer with either 4 or 5 neurons per layer show good results, so both will be visualized.

```{r}
nnFour= neuralnet( Species ~Sepal.Length+Sepal.Width+Petal.Length,data=train, 
                     hidden=4,
                     act.fct = "logistic",linear.output = FALSE,rep=2,threshold = 0.5)
```

```{r}
plot(nnFour,1)
```

```{r}
nnFive= neuralnet( Species ~Sepal.Length+Sepal.Width+Petal.Length,data=train, 
                     hidden=5,
                     act.fct = "logistic",linear.output = FALSE,rep=2,threshold = 0.5)
plot(nnFive,1)
```


# 5 Use the best model with the winning parameters to evaluate test loss and interpret the confusion matrix.
Both, one hidden layer with either 4 or 5 neurons per layer show good results, so both will be evaluated.

Real Species
```{r}
real <- test %>%  
  mutate(Species= ifelse(Species=="setosa",
                         1,ifelse(Species=="versicolor",2,ifelse(Species=="virginica",3,NA))))
real<- as.factor(real[,"Species"])
```

```{r}
prediction1 <- neuralnet::compute(nnFour, (test %>% select(-Species)))
prediction1 <- max.col(prediction1$net.result) %>%  as.factor()
prediction1 <- factor(prediction1,levels = c(1,2,3))

confusionMatrix(real,prediction1)
```

```{r}
prediction2 <- neuralnet::compute(nnFive, (test %>% select(-Species)))
prediction2 <- max.col(prediction2$net.result) %>%  as.factor()
prediction2 <- factor(prediction2,levels = c(1,2,3))

confusionMatrix(real,prediction2)
```
## Interpretatin Confusion Matrix

The NN with 5 Neuros seems to have a overall better Accuracy (True/False Ratio). 
However, the NN with 4 Neurons seems to have better Senstivity regarding Class 1 and Class 2 but a worse Senstivity towards Class 3.
Including senstivity from both matrizes, data suggest that the both NN are good in indentifying class 1 cases, but the NN wiht 4 Neurons in the hidden layer tends to (wrongly) classfies more cases as Class 2.

# 6 Discuss the problem of overfitting in neural networks and how you can spot it as well as prevent it from happening.


Standard way to avoid overfitting is by applying cross validation.

https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a
https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/

